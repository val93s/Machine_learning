{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/val93s/Machine_learning/blob/main/Copy_of_11_13_3_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "# Activity 11.13.3\n",
        "\n",
        "The sheer volume of text data that has been written—and that is written every day—makes automatic text summary very useful. In this activity, you'll summarize the text of [two articles written by Frederick Douglass](https://www.gutenberg.org/cache/epub/99/pg99.txt) using extractive summarization. Note that, in the nineteenth century, when these articles were written, it was common to write very (very!) long sentences. You'll see that reflected in some of the summaries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Install the Necessary Packages\n",
        "* Run the following code block to import the necessary libraries and packages:"
      ],
      "metadata": {
        "id": "eOE0IPoIQ91v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1\n",
        "\n",
        "!pip install gensim\n",
        "import gensim\n",
        "import re\n",
        "import requests\n",
        "\n",
        "!pip install sumy\n",
        "import sumy\n",
        "\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "nFZnI6vBp8fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDbdDBDQVoS"
      },
      "source": [
        "#Step 2: Download and Save the Text Files for This Activity\n",
        "* Download `Douglass Article 1.txt` and `Douglass Article 2.txt`from the class materials.  \n",
        "* Make a note of where you saved the file on your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63UbfVKeQfsl"
      },
      "source": [
        "#Step 3: Upload the Text Files by Running the Following Code Block \n",
        "* Run the code block twice to upload both documents.\n",
        "* When prompted, navigate to and select each text file from where you saved it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxSC_MgQQUis"
      },
      "source": [
        "#Step 3\n",
        "\n",
        "from google.colab import files\n",
        "text = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Open and Process the Two Articles\n",
        "* Run the following code block to open the two text files and remove unnecessary characters and punctuation. \n",
        "* The code block will print the text of each article in one long string.  You can more easily read the text of both articles on [Project Gutenberg](https://www.gutenberg.org/cache/epub/99/pg99.txt)."
      ],
      "metadata": {
        "id": "q0dLzWPaDWrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4\n",
        "\n",
        "with open('Douglass Article 1.txt') as f:\n",
        "    text1 = f.readlines()\n",
        "\n",
        "with open('Douglass Article 2.txt') as f:\n",
        "    text2 = f.readlines()\n",
        "\n",
        "article1 = ''\n",
        "for line in text1: \n",
        "  article1 +=line\n",
        "\n",
        "article2 = ''\n",
        "for line in text2: \n",
        "  article2 +=line\n",
        "\n",
        "article1 = re.sub('[^a-zA-Z\\.]', ' ', article1)\n",
        "article1 = re.sub(r'\\s+', ' ', article1)\n",
        "\n",
        "article2 = re.sub('[^a-zA-Z\\.]', ' ', article2)\n",
        "article2 = re.sub(r'\\s+', ' ', article2)\n",
        "\n",
        "print(article1)\n",
        "print(article2)"
      ],
      "metadata": {
        "id": "mYAcafy07spe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Tokenize Both Articles\n",
        "* Run the following code block to tokenize the two articles: "
      ],
      "metadata": {
        "id": "pMwParaJHEM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article1_parsed = PlaintextParser.from_string(article1,Tokenizer('english'))\n",
        "\n",
        "article2_parsed = PlaintextParser.from_string(article2,Tokenizer('english'))"
      ],
      "metadata": {
        "id": "RHUvRqyZ88f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Use the LexRank Summarizer to Summarize Article 1 in Three Sentences \n",
        "* Run the following code block to print the three-sentence summary.\n",
        "* Feel free to adjust the length of the summary by changing the value of `sentences_count` in the `lex_rank_summarizer` to see which sentences are chosen for the summary."
      ],
      "metadata": {
        "id": "o4HTs6FaIZ_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a summary of 3 sentences.\n",
        "lex_rank_summarizer = LexRankSummarizer()\n",
        "lexrank_summary = lex_rank_summarizer(article1_parsed.document,sentences_count=3)\n",
        "\n",
        "lexrank_summary"
      ],
      "metadata": {
        "id": "KanQWk469Llj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Use the LexRank Summarizer to Summarize Article 2 in Three Sentences \n",
        "* Run the following code block to print the three-sentence summary.\n",
        "* Feel free to adjust the length of the summary by changing the value of `sentences_count` in the `lex_rank_summarizer` to see which sentences are chosen for the summary."
      ],
      "metadata": {
        "id": "7w09KFirJU28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a summary of 3 sentences.\n",
        "lex_rank_summarizer = LexRankSummarizer()\n",
        "lexrank_summary = lex_rank_summarizer(article2_parsed.document,sentences_count=3)\n",
        "\n",
        "lexrank_summary"
      ],
      "metadata": {
        "id": "2ccCTgT3F460"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 8: Evaluate the Summaries\n",
        "* If you haven't yet, read through the text of at least one of [Douglass's articles](https://www.gutenberg.org/cache/epub/99/pg99.txt).\n",
        "* Do you think that the summarizer did a good job selecting the most important sentences for the summary?"
      ],
      "metadata": {
        "id": "HdbxIS4XKF3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8 Answer:**\n",
        "\n"
      ],
      "metadata": {
        "id": "LpyEAXd8Kb39"
      }
    }
  ]
}